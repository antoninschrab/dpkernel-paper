"""
The Test of Tests: A Framework For Differentially Private Hypothesis Testing
Zeki Kazan, Kaiyan Shi, Adam Groce, Andrew Bray
https://arxiv.org/abs/2302.04260
"""


import jax
import jax.numpy as jnp
from jax import vmap, random, jit, lax
from jax.flatten_util import ravel_pytree
from functools import partial
import numpy as np
import math
from pval import compute_p_val_mmd, compute_p_val_hsic


@partial(jit, static_argnums=(5, 8, 9, 10))
def tot_mmd(
    key,
    X,
    Y,
    epsilon,
    alpha=0.05,
    S=10,
    alpha0=0.05,
    bandwidth_multiplier=1,
    kernel="gaussian",
    number_permutations=2000, 
    min_mem_kernel=False,
):
    """
    Differential Privacy Two-Sample TOT MMD test.
     
    Given data from one distribution and data from another distribution,
    return 0 if the test fails to reject the null 
        (i.e. data comes from the same distribution), 
    or return 1 if the test rejects the null 
        (i.e. data comes from different distributions).
    
    Fixing the two sample sizes and the dimension, the first time the function is
    run it is getting compiled. After that, the function can fastly be evaluated on 
    any data with the same sample sizes and dimension.
    
    Parameters
    ----------
    key:
        Jax random key (can be generated by jax.random.PRNGKey(seed) for an integer seed).
    X : array_like
        The shape of X must be of the form (m, d) where m is the number
        of samples and d is the dimension.
    Y: array_like
        The shape of X must be of the form (n, d) where n is the number
        of samples and d is the dimension.
    epsilon: scalar
        Differential privacy level (positive scalar).
    alpha: scalar
        The value of test level alpha must be between 0 and 1.
    S: int
        Number of subsets for data splitting (between 1 and min(n, m)).
    alpha0: scalar
        The value of sub-test level alpha0 must be between 0 and 1.
    bandwidth_multiplier: scalar
        Bandwidth for l1 kernels is d * bandwidth_multiplier.
        Bandwidth for l2 kernels is sqrt(d) * bandwidth_multiplier.
        So that the l1 and l2 norms are of constant order wrt dimension.
    kernel: str
        The value of kernel must be "gaussian" or "laplace" or "imq".
    number_permutations: int
        Number of permutations to approximate the quantiles. 
    min_mem_kernel: bool
        If True then compute kernel matrix sequentially (lower memory).
        The speed improvement can vary depending on the use of CPU/GPU.
        
    Returns
    -------
    output: int
        0 if the TOT MMD test fails to reject the null 
            (i.e. data comes from the same distribution)
        1 if the TOT MMD test rejects the null 
            (i.e. data comes from different distributions)
    """    
    def compute_p_val_mmd_tuple(tuple_X_Y_key):
        X, Y, key = tuple_X_Y_key
        return compute_p_val_mmd(
            key=key,
            X=X,
            Y=Y,
            bandwidth_multiplier=bandwidth_multiplier,
            kernel=kernel,
            number_permutations=number_permutations, 
            min_mem_kernel=min_mem_kernel,
        )

    return tot(
        compute_p_val=compute_p_val_mmd_tuple,
        key=key,
        X=X,
        Y=Y,
        epsilon=epsilon,
        alpha=alpha,
        S=S,
        alpha0=alpha0,
    )
    

@partial(jit, static_argnums=(5, 9, 10, 11, 12))
def tot_hsic(
    key,
    X,
    Y,
    epsilon,
    alpha=0.05,
    S=10,
    alpha0=0.05,
    bandwidth_multiplier_X=1,
    bandwidth_multiplier_Y=1,
    kernel_X="gaussian",
    kernel_Y="gaussian",
    number_permutations=2000, 
    min_mem_kernel=False,
):
    """
    Differential Privacy Independence TOT HSIC test.
     
    Given paired data from a joint distribution,
    return 0 if the test fails to reject the null 
        (i.e. paired data is independent), 
    or return 1 if the test rejects the null 
        (i.e. paired data is dependent).
    
    Fixing the sample size and the dimension, the first time the function is
    run it is getting compiled. After that, the function can fastly be evaluated on 
    any data with the same sample size and dimension.
    
    Parameters
    ----------
    key:
        Jax random key (can be generated by jax.random.PRNGKey(seed) for an integer seed).
    X : array_like
        The shape of X must be of the form (m, d) where m is the number
        of samples and d is the dimension.
    Y: array_like
        The shape of X must be of the form (n, d) where n is the number
        of samples and d is the dimension.
    epsilon: scalar
        Differential privacy level (positive scalar).
    alpha: scalar
        The value of test level alpha must be between 0 and 1.
    S: int
        Number of subsets for data splitting (between 1 and min(n, m)).
    alpha0: scalar
        The value of sub-test level alpha0 must be between 0 and 1.
    bandwidth_multiplier_X: scalar
        Bandwidth for l1 X-kernels is d_X * bandwidth_multiplier.
        Bandwidth for l2 X-kernels is sqrt(d_X) * bandwidth_multiplier.
        So that the l1 and l2 norms are of constant order wrt dimension.
    bandwidth_multiplier_Y: scalar
        Bandwidth for l1 Y-kernels is d_Y * bandwidth_multiplier.
        Bandwidth for l2 Y-kernels is sqrt(d_Y) * bandwidth_multiplier.
        So that the l1 and l2 norms are of constant order wrt dimension.
    kernel_X: str
        The value of kernel_X for X must be "gaussian", "laplace", "imq"
    kernel_Y: str
        The value of kernel_Y for Y must be "gaussian", "laplace", "imq"
    number_permutations: int
        Number of permutations to approximate the quantiles.
    min_mem_kernel: bool
        If True then compute kernel matrix sequentially (lower memory).
        The speed improvement can vary depending on the use of CPU/GPU.
        
    Returns
    -------
    output: int
        0 if the TOT HSIC test fails to reject the null 
            (i.e. paired data is independent)
        1 if the TOT HSIC test rejects the null 
            (i.e. paired data is dependent)
    """   
    def compute_p_val_hsic_tuple(tuple_X_Y_key):
        X, Y, key = tuple_X_Y_key
        return compute_p_val_hsic(
            key=key,
            X=X,
            Y=Y,
            bandwidth_multiplier_X=bandwidth_multiplier_X,
            bandwidth_multiplier_Y=bandwidth_multiplier_Y,
            kernel_X=kernel_X,
            kernel_Y=kernel_Y,
            number_permutations=number_permutations, 
            min_mem_kernel=min_mem_kernel,
        )

    return tot(
        compute_p_val=compute_p_val_hsic_tuple,
        key=key,
        X=X,
        Y=Y,
        epsilon=epsilon,
        alpha=alpha,
        S=S,
        alpha0=alpha0,
    )


def tot(
    compute_p_val,    
    key,
    X,
    Y,
    epsilon,
    alpha,
    S,
    alpha0,
):
    """
    Function used to define tot_mmd and tot_hsic.
    Refer to the docstring for these functions.
    """
    # Assertions
    m = X.shape[0]
    n = Y.shape[0]
    d_X = X.shape[1]
    d_Y = Y.shape[1]
    assert n >= 2 and m >= 2
    
    # bin the X data
    index_X = int(S * math.floor(m / S))
    X_S_bins = X[:index_X].reshape(S, int(math.floor(m / S)), d_X)

    # bin the Y data
    index_Y = int(S * math.floor(n / S))
    Y_S_bins = Y[:index_Y].reshape(S, int(math.floor(n / S)), d_Y)
    
    # Compute p-values for the sub tests
    key, *subkeys = random.split(key, num=S + 1)
    sub_p_vals = lax.map(compute_p_val, (X_S_bins, Y_S_bins, jnp.array(subkeys)))  # (S, )

    # Number of rejects
    a = jnp.sum(sub_p_vals <= alpha0)
    
    # Privatize a using Tulap noise 
    # Awan and SlavkoviÄ‡, 2018, Algorithm 2
    p = 1 - jnp.exp(-epsilon)
    # Geometric distribution
    key, subkey = random.split(key)
    u_g1 = jax.random.uniform(subkey)
    g1 = jnp.ceil(jnp.log(u_g1) / jnp.log1p(-p))
    key, subkey = random.split(key)
    u_g2 = jax.random.uniform(subkey)
    g2 = jnp.ceil(jnp.log(u_g2) / jnp.log1p(-p))
    # Uniform(-0.5, 0.5) distribution
    key, subkey = random.split(key)
    u = jax.random.uniform(subkey) - 0.5
    # Privatize a
    z = a + g1 - g2 + u
    
    # CDF Tulap 
    def cdf_tulap(x, b):
        s = jnp.sign(x)
        r = jnp.round(x)
        output = (s + 1) / 2 - s / (b + 1) * b ** (s * r) * (b + (0.5 + s * (r - x)) * (1 - b) )
        return output
    
    # Compute the p-value (Awan and SlavkoviÄ‡, 2018, Algorithm 1)
    binomal_values = jnp.array([np.math.factorial(S) / (np.math.factorial(s) * np.math.factorial(S - s)) * alpha0 ** s * (1 - alpha0) ** (S - s) for s in range(S + 1)])
    cdf_tulap_values = jnp.array([cdf_tulap(s - z, jnp.exp(-epsilon)) for s in range(S + 1)])
    p_val = binomal_values @ cdf_tulap_values
    
    return (p_val <= alpha).astype(int)
