import jax
import jax.numpy as jnp
from jax import random, lax
from kernel import kernel_matrix, distances


def compute_p_val_mmd(
    key,
    X,
    Y,
    bandwidth_multiplier=1,
    kernel="gaussian",
    number_permutations=2000, 
    min_mem_kernel=False,
):
    """
    Compute p-value of the non-private Two-Sample MMD test.
    
    Parameters
    ----------
    key:
        Jax random key (can be generated by jax.random.PRNGKey(seed) for an integer seed).
    X: array_like
        The shape of X must be of the form (m, d) where m is the number
        of samples and d is the dimension.
    Y: array_like
        The shape of Y must be of the form (n, d) where n is the number
        of samples and d is the dimension.
    bandwidth_multiplier: scalar
        Bandwidth for l1 kernels is d * bandwidth_multiplier.
        Bandwidth for l2 kernels is sqrt(d) * bandwidth_multiplier.
        So that the l1 and l2 norms are of constant order wrt dimension.
    kernel: str
        The value of kernel must be "gaussian" or "laplace" or "imq".
    number_permutations: int
        Number of permutations to approximate the quantiles.
    min_mem_kernel: bool
        If True then compute kernel matrix sequentially (lower memory).
        The speed improvement can vary depending on the use of CPU/GPU.
        
    Returns
    -------
    output: scalar
        p-value between 0 and 1.
        Level-alpha test is rejected if the p-values is smaller than alpha.
    """
    # Assertions
    B = number_permutations
    d = X.shape[1]
    m = X.shape[0]
    n = Y.shape[0]
    assert n >= 2 and m >= 2
    assert X.shape[1] == Y.shape[1]
    assert kernel in ("gaussian", "laplace", "imq")
    assert B > 0 and type(B) == int
    l = "l2" if kernel in ("gaussian", "imq") else "l1"
    d_scale = d if l == 'l1' else jnp.sqrt(d)
    bandwidth = bandwidth_multiplier * d_scale

    # Setup for permutations
    key, subkey = random.split(key)
    # (B+1, m+n): rows of permuted indices
    idx = random.permutation(subkey, jnp.array([[i for i in range(m + n)]] * (B + 1)), axis=1, independent=True)   
    v = jnp.concatenate((jnp.ones(m) / m, - jnp.ones(n) / n))  # (m+n, )
    V_stack = jnp.tile(v, (B + 1, 1))  # (B+1, m+n)
    V = jnp.take_along_axis(V_stack, idx, axis=1)  # (B+1, m+n): permute the entries of the rows
    V = V.at[B].set(v) # (B+1)th entry is the original MMD (no permutation)
    V = V.transpose()  # (m+n, B+1)

    # Compute all simulated MMD estimates and quantile
    Z = jnp.concatenate((X, Y))
    pairwise_matrix = distances(Z, Z, l, matrix=True, min_mem=min_mem_kernel)
    K = kernel_matrix(pairwise_matrix, l, kernel, bandwidth)
    # compute MMD permuted values (B + 1, )
    mmd_values = jnp.sqrt(jnp.sum(V * (K @ V), 0))
    mmd_original = mmd_values[B]
    # p-value
    p_val = jnp.mean(mmd_values >= mmd_original)
    return p_val


def compute_p_val_hsic(
    key,
    X,
    Y,
    bandwidth_multiplier_X=1,
    bandwidth_multiplier_Y=1,
    kernel_X="gaussian",
    kernel_Y="gaussian",
    number_permutations=2000, 
    min_mem_kernel=False,
):
    """
    Compute p-value of the non-private Independence HSIC test.
    
    Parameters
    ----------
    key:
        Jax random key (can be generated by jax.random.PRNGKey(seed) for an integer seed).
    X : array_like
        The shape of X must be of the form (n, d_X) where m is the number
        of samples and d is the dimension.
    Y: array_like
        The shape of Y must be of the form (n, d_Y) where m is the number
        of samples and d is the dimension.
    bandwidth_multiplier_X: scalar
        Bandwidth for l1 X-kernels is d_X * bandwidth_multiplier.
        Bandwidth for l2 X-kernels is sqrt(d_X) * bandwidth_multiplier.
        So that the l1 and l2 norms are of constant order wrt dimension.
    bandwidth_multiplier_Y: scalar
        Bandwidth for l1 Y-kernels is d_Y * bandwidth_multiplier.
        Bandwidth for l2 Y-kernels is sqrt(d_Y) * bandwidth_multiplier.
        So that the l1 and l2 norms are of constant order wrt dimension.
    kernel_X: str
        The value of kernel_X for X must be "gaussian", "laplace", "imq"
    kernel_Y: str
        The value of kernel_Y for Y must be "gaussian", "laplace", "imq"
    number_permutations: int
        Number of permutations to approximate the quantiles.
    min_mem_kernel: bool
        If True then compute kernel matrix sequentially (lower memory).
        The speed improvement can vary depending on the use of CPU/GPU.
        
    Returns
    -------
    output: scalar
        p-value between 0 and 1.
        Level-alpha test is rejected if the p-values is smaller than alpha.
    """
    # Assertions
    B = number_permutations
    assert X.shape[0] == Y.shape[0]
    n = X.shape[0]
    d_X = X.shape[1]
    d_Y = Y.shape[1]
    assert n >= 2
    assert kernel_X in ("gaussian", "laplace", "imq")
    assert kernel_Y in ("gaussian", "laplace", "imq")
    assert B > 0 and type(B) == int
    l_X = "l2" if kernel_X in ("gaussian", "imq") else "l1"
    d_scale_X = d_X if l_X == 'l1' else jnp.sqrt(d_X)
    bandwidth_X = bandwidth_multiplier_X * d_scale_X
    l_Y = "l2" if kernel_Y in ("gaussian", "imq") else "l1"
    d_scale_Y = d_Y if l_Y == 'l1' else jnp.sqrt(d_Y)
    bandwidth_Y = bandwidth_multiplier_Y * d_scale_Y

    # Setup for permutations
    key, subkey = random.split(key)
    # (B+1, n): rows of permuted indices
    idx = random.permutation(subkey, jnp.array([[i for i in range(n)]] * (B + 1)), axis=1, independent=True)  
    idx = idx.at[B].set(jnp.array([i for i in range(n)]))

    # compute both kernel matrices
    pairwise_matrix_X = distances(X, X, l_X, matrix=True, min_mem=min_mem_kernel)
    K = kernel_matrix(pairwise_matrix_X, l_X, kernel_X, bandwidth_X)
    pairwise_matrix_Y = distances(Y, Y, l_Y, matrix=True, min_mem=min_mem_kernel)
    L = kernel_matrix(pairwise_matrix_Y, l_Y, kernel_Y, bandwidth_Y)

    # center kernel matrix L (HLH for H = I - 1 @ 1.T / n)
    center_rows = lambda mat : mat - mat.mean(1).reshape(-1, 1)
    center_columns = lambda mat : mat - mat.mean(0)
    L = center_rows(center_columns(L))
    
    # compute HSIC permuted values (B + 1, )
    compute_hsic = lambda index : jnp.sqrt(jnp.sum(K[index][:, index] * L) / n ** 2)
    hsic_values = lax.map(compute_hsic, idx)  # (B + 1, )
    hsic_original = hsic_values[B]

    # p-value
    p_val = jnp.mean(hsic_values >= hsic_original)
    return p_val