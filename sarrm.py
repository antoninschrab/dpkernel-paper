"""
Differentially Private Hypothesis Testing with the Subsampled and Aggregated Randomized Response Mechanism
Victor Pena, Andres F. Barrientos
https://arxiv.org/abs/2208.06803
"""


import jax
import jax.numpy as jnp
from jax import vmap, random, jit, lax
from jax.flatten_util import ravel_pytree
from functools import partial
import numpy as np
import math
from pval import compute_p_val_mmd, compute_p_val_hsic
import scipy


@partial(jit, static_argnums=(3, 7, 8, 9))
def sarrm_mmd(
    key,
    X,
    Y,
    k,
    p,
    alpha0,
    bandwidth_multiplier=1,
    kernel="gaussian",
    number_permutations=2000,
    min_mem_kernel=False,
):
    """
    Differential Privacy Two-Sample SARRM MMD test.
     
    Given data from one distribution and data from another distribution,
    return 0 if the test fails to reject the null 
        (i.e. data comes from the same distribution), 
    or return 1 if the test rejects the null 
        (i.e. data comes from different distributions).
    
    Fixing the two sample sizes and the dimension, the first time the function is
    run it is getting compiled. After that, the function can fastly be evaluated on 
    any data with the same sample sizes and dimension.
    
    Parameters
    ----------
    key:
        Jax random key (can be generated by jax.random.PRNGKey(seed) for an integer seed).
    X : array_like
        The shape of X must be of the form (m, d) where m is the number
        of samples and d is the dimension.
    Y: array_like
        The shape of X must be of the form (n, d) where n is the number
        of samples and d is the dimension.
    k: int
        Number determining the number of subsets S = 2k + 1 for data splitting.
        Can be determined by running compute_optimal_k_alpha0_p(alpha, epsilon).
    p: scalar
        Probability parameter between 0.5 and 1.
        Can be determined by running compute_optimal_k_alpha0_p(alpha, epsilon).
    alpha0: scalar
        The value of sub-test level alpha0 must be between 0 and 1.
        Can be determined by running compute_optimal_k_alpha0_p(alpha, epsilon).
    bandwidth_multiplier: scalar
        Bandwidth for l1 kernels is d * bandwidth_multiplier.
        Bandwidth for l2 kernels is sqrt(d) * bandwidth_multiplier.
        So that the l1 and l2 norms are of constant order wrt dimension.
    kernel: str
        The value of kernel must be "gaussian" or "laplace" or "imq".
    number_permutations: int
        Number of permutations to approximate the quantiles.
    min_mem_kernel: bool
        If True then compute kernel matrix sequentially (lower memory).
        The speed improvement can vary depending on the use of CPU/GPU.
        
    Returns
    -------
    output: int
        0 if the SARRM MMD test fails to reject the null 
            (i.e. data comes from the same distribution)
        1 if the SARRM MMD test rejects the null 
            (i.e. data comes from different distributions)
    """    
    def compute_p_val_mmd_tuple(tuple_X_Y_key):
        X, Y, key = tuple_X_Y_key
        return compute_p_val_mmd(
            key=key,
            X=X,
            Y=Y,
            bandwidth_multiplier=bandwidth_multiplier,
            kernel=kernel,
            number_permutations=number_permutations, 
            min_mem_kernel=min_mem_kernel,
        )

    return sarrm(
        compute_p_val=compute_p_val_mmd_tuple,
        key=key,
        X=X,
        Y=Y,
        k=k,
        p=p,
        alpha0=alpha0,
    )
    

@partial(jit, static_argnums=(3, 8, 9, 10, 11))
def sarrm_hsic(
    key,
    X,
    Y,
    k,
    p,
    alpha0,
    bandwidth_multiplier_X=1,
    bandwidth_multiplier_Y=1,
    kernel_X="gaussian",
    kernel_Y="gaussian",
    number_permutations=2000,
    min_mem_kernel=False,
):
    """
    Differential Privacy Independence SARRM HSIC test.
     
    Given paired data from a joint distribution,
    return 0 if the test fails to reject the null 
        (i.e. paired data is independent), 
    or return 1 if the test rejects the null 
        (i.e. paired data is dependent).
    
    Fixing the sample size and the dimension, the first time the function is
    run it is getting compiled. After that, the function can fastly be evaluated on 
    any data with the same sample size and dimension.
    
    Parameters
    ----------
    key:
        Jax random key (can be generated by jax.random.PRNGKey(seed) for an integer seed).
    X : array_like
        The shape of X must be of the form (n, d_X) where n is the number
        of samples and d_X is the dimension.
    Y: array_like
        The shape of X must be of the form (n, d_Y) where n is the number
        of samples and d_Y is the dimension.
    k: int
        Number determining the number of subsets S = 2k + 1 for data splitting.
        Can be determined by running compute_optimal_k_alpha0_p(alpha, epsilon).
    p: scalar
        Probability parameter between 0.5 and 1.
        Can be determined by running compute_optimal_k_alpha0_p(alpha, epsilon).
    alpha0: scalar
        The value of sub-test level alpha0 must be between 0 and 1.
        Can be determined by running compute_optimal_k_alpha0_p(alpha, epsilon).
    bandwidth_multiplier_X: scalar
        Bandwidth for l1 X-kernels is d_X * bandwidth_multiplier.
        Bandwidth for l2 X-kernels is sqrt(d_X) * bandwidth_multiplier.
        So that the l1 and l2 norms are of constant order wrt dimension.
    bandwidth_multiplier_Y: scalar
        Bandwidth for l1 Y-kernels is d_Y * bandwidth_multiplier.
        Bandwidth for l2 Y-kernels is sqrt(d_Y) * bandwidth_multiplier.
        So that the l1 and l2 norms are of constant order wrt dimension.
    kernel_X: str
        The value of kernel_X for X must be "gaussian", "laplace", "imq"
    kernel_Y: str
        The value of kernel_Y for Y must be "gaussian", "laplace", "imq"
    number_permutations: int
        Number of permutations to approximate the quantiles.
    min_mem_kernel: bool
        If True then compute kernel matrix sequentially (lower memory).
        The speed improvement can vary depending on the use of CPU/GPU.
        
    Returns
    -------
    output: int
        0 if the SARRM HSIC test fails to reject the null 
            (i.e. paired data is independent)
        1 if the SARRM HSIC test rejects the null 
            (i.e. paired data is dependent)
    """   
    def compute_p_val_hsic_tuple(tuple_X_Y_key):
        X, Y, key = tuple_X_Y_key
        return compute_p_val_hsic(
            key=key,
            X=X,
            Y=Y,
            bandwidth_multiplier_X=bandwidth_multiplier_X,
            bandwidth_multiplier_Y=bandwidth_multiplier_Y,
            kernel_X=kernel_X,
            kernel_Y=kernel_Y,
            number_permutations=number_permutations, 
            min_mem_kernel=min_mem_kernel,
        )

    return sarrm(
        compute_p_val=compute_p_val_hsic_tuple,
        key=key,
        X=X,
        Y=Y,
        k=k,
        p=p,
        alpha0=alpha0,
    )


def sarrm(
    compute_p_val,
    key,
    X,
    Y,
    k,
    p,
    alpha0,
):
    """
    Function used to define tot_mmd and tot_hsic.
    Refer to the docstring for these functions.
    """
    # Assertions
    m = X.shape[0]
    n = Y.shape[0]
    d_X = X.shape[1]
    d_Y = Y.shape[1]
    assert n >= 2 and m >= 2
    
    # Number of subsets
    S = 2 * k + 1
    if 2 * k + 1 >= np.minimum(m, n):
        # the test can not be run with the 
        return -1
    
    # bin the X data
    index_X = int(S * math.floor(m / S))
    X_S_bins = X[:index_X].reshape(S, int(math.floor(m / S)), d_X)

    # bin the Y data
    index_Y = int(S * math.floor(n / S))
    Y_S_bins = Y[:index_Y].reshape(S, int(math.floor(n / S)), d_Y)
    
    # Compute p-values for the sub tests
    key, *subkeys = random.split(key, num=S + 1)
    sub_p_vals = lax.map(compute_p_val, (X_S_bins, Y_S_bins, jnp.array(subkeys)))  # (S, )

    # Randomized Response Mechanism
    def rrm(tuple_x_key):
        x, key = tuple_x_key
        b = jax.random.bernoulli(key, p).astype(int)  # 1 with probability p
        return b * x + (1 - b) * (1 - x)
        
    # Statistic
    sub_outputs = (sub_p_vals <= alpha0).astype(int)
    key, *subkeys = random.split(key, num=S + 1)
    rrm_sub_outputs = lax.map(rrm, (sub_outputs, jnp.array(subkeys)))  # (S, )
    T = jnp.sum(rrm_sub_outputs)
    
    # Reject H_0 is T > k
    return (T > k).astype(int)


def compute_sarrm_optimal_k_p_alpha0(alpha, epsilon, verbose=False):
    """
    Given the test level alpha and privacy parameter epsilon,
    determine the parameters required to run the SARRM MMD and HSIC tests.
        
    Parameters
    ----------
    alpha: scalar
        The value of test level alpha must be between 0 and 1. 
    epsilon: scalar
        Differential privacy level (positive scalar).
        
    Returns
    -------
    output: tuple of length 3 with elements corresponding to
    the three parameters of sarrm_hsic and sarrm_mmd:
        k: int
            Number determining the number of subsets S = 2k + 1 for data splitting.
        p: scalar
            Probability parameter between 0.5 and 1.
        alpha0: scalar
            The value of sub-test level alpha0 must be between 0 and 1.
    """  
    
    def log_binom_proba_strictly_greater(k, S, proba):
        return scipy.special.logsumexp(
            [
                np.sum(np.log(np.arange(S) + 1))
                - np.sum(np.log(np.arange(l) + 1))
                - np.sum(np.log(np.arange(S - l) + 1))
                + l * np.log(proba)
                + (S - l)  * np.log(1 - proba)
                for l in range(k + 1, S + 1)
            ]
        )        
    
    q = lambda alpha0, p : p * alpha0 + (1 - p) * (1 - alpha0)
    
    def compute_alpha_lower(k, number_steps_bissection):

        # choose p satisfying DP property
        # Proposition 2 of Pena and Barrientos 2022
        log_proba_B1 = lambda k, p : scipy.special.logsumexp(
            [
                log_binom_proba_strictly_greater(k, 2 * k, 1 - p) + np.log(1 - p),
                 log_binom_proba_strictly_greater(k - 1, 2 * k, 1 - p) + np.log(p)
            ]
        )
        log_proba_B0 = lambda k, p : log_binom_proba_strictly_greater(k, 2 * k + 1, 1 - p)
        privacy = lambda k, p : log_proba_B1(k, p) - log_proba_B0(k, p)
        
        # for the given k
        # select p so that 
        # privacy >= epsilon 
        # using a bissection method
        p_max = 0.99999999999999
        p_min = 0.5
        for _ in range(number_steps_bissection): 
            p = (p_min + p_max) / 2
            epsilon_p = privacy(k, p)
            if epsilon_p <= epsilon:
                p_min = p
            else:
                p_max = p
            
        p = p_max
        epsilon_p = privacy(k, p)
        assert not bool(np.isnan(epsilon_p)), 'NaN value encountered in epsilon_p.'
        if verbose:
            print('p', p)
            print('epsilon_p', epsilon_p)
            print(' ')
        
        alpha0_min = 0.0025
        log_alpha_lower = log_binom_proba_strictly_greater(k, 2 * k + 1, float(q(alpha0_min, p)))
        
        return log_alpha_lower, p
    
    
    log_alpha = np.log(alpha)
    
    k_max = 2500  # maximum sample size we consider is 5000
    
    if verbose:
        print('bissection k', k_max)
    log_alpha_lower, p = compute_alpha_lower(k_max, number_steps_bissection=20)
    if log_alpha_lower > log_alpha:
        # we will not be able to run the test
        if verbose:
            print("Warning. Value of k is required to be larger than 2500, we return k = 2500 and arbitrary p and alpha0.")
        return k_max, 0.7, 0.05
    
    # perform bissection method
    k_min = 1
    k_max = 2500
    # at most 11 steps as 1500 / 2 ** 12 < 1
    while True: 
            k = int((k_min + k_max) / 2)
            if k == k_min: 
                break
            if verbose:
                print('bissection k', k)
            log_alpha_lower, p = compute_alpha_lower(k, number_steps_bissection=20)
            if log_alpha_lower > log_alpha:
                k_min = k
            else:
                k_max = k
    assert k_max == k_min + 1
    k = k_max
    if verbose: 
        print('final k', k)
    
    # for the given k compute p
    log_alpha_lower, p = compute_alpha_lower(k, number_steps_bissection=20)
    if verbose: 
        print('final k', k)
        print('final p', p)
        
    # for the given k and p
    # select alpha0 so that 
    # binom_proba_strictly_greater(k, 2 * k + 1, q(alpha0, p)) \approx alpha
    # using a bissection method
    alpha0_max = 1.
    alpha0_min = 0.0025
    number_steps_bissection = 20
    for _ in range(number_steps_bissection): 
        alpha0 = (alpha0_min + alpha0_max) / 2
        log_level = log_binom_proba_strictly_greater(k, 2 * k + 1, float(q(alpha0, p)))
        if log_level <= np.log(alpha):
            alpha0_min = alpha0
        else: 
            alpha0_max = alpha0
    alpha0 = alpha0_min
    log_level = log_binom_proba_strictly_greater(k, 2 * k + 1, q(alpha0, p))

    assert not bool(np.isnan(log_level)), 'NaN value encountered in level.'
    if verbose:
        print('final alpha0', alpha0)
        print('final level', np.exp(log_level))
        
    return k, p, alpha0
